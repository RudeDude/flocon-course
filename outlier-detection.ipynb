{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"data/two-hour-sample.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(inputFile)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns because when I did this work I liked my names better\n",
    "colnames = [\"StartTime\", \"Dur\", \"Proto\", \"SrcAddr\", \"Sport\", \"Dir\", \"DstAddr\",\n",
    "            \"Dport\", \"TotPkts\", \"TotBytes\", \"SrcBytes\"]\n",
    "df = df[colnames]\n",
    "df.columns = ['timestamp', 'duration', 'proto', 'src_ip', 'src_port', 'direction', 'dest_ip',\n",
    "                'dest_port', 'tot_pkts', 'tot_bytes', 'bytes_toclient']\n",
    "df['row_id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up missing ports\n",
    "df.src_port.fillna(0)\n",
    "df.src_port.fillna(0)\n",
    "df.replace(to_replace={'src_port': {float('NaN'): 0},\n",
    "                        'dest_port': {float('NaN'): 0}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a place holder for the example, normally this would be extracted from the timestamp\n",
    "df['day'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add Total Counts (How much overall traffic to this IP?)\n",
    "totalCount = df.shape[0]\n",
    "\n",
    "srcDf = df[['src_ip', 'proto']].groupby(\n",
    "    'src_ip', as_index=False).count().rename({\"proto\": \"src_count\"}, axis=1)\n",
    "print(srcDf.head())\n",
    "\n",
    "destDf = df[['dest_ip', 'proto']].groupby(\n",
    "    'dest_ip', as_index=False).count().rename({\"proto\": \"dest_count\"}, axis=1)\n",
    "print(destDf.head())\n",
    "\n",
    "src_joined = pd.merge(df, srcDf, how='left',\n",
    "                        on='src_ip', suffixes=('', '_count'))\n",
    "df2 = pd.merge(src_joined, destDf, how='left', on=[\n",
    "                'dest_ip'], suffixes=('', '_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compute IP percentages\n",
    "srcCol = df2.columns.get_loc('src_count')\n",
    "destCol = df2.columns.get_loc('dest_count')\n",
    "\n",
    "print(str(srcCol) + \" \" + str(destCol))\n",
    "dfa = df2.assign(src_pct=df2.src_count / totalCount)\n",
    "dfb = dfa.assign(dest_pct=dfa.dest_count / totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute Protocol Percentages\n",
    "\n",
    "srcDf = dfb[['src_ip', 'proto', \"day\"]].groupby(\n",
    "    ['src_ip', 'proto'], as_index=False).count().rename({\"day\": \"src_proto_count\"}, axis=1)\n",
    "# print(srcDf.head())\n",
    "\n",
    "destDf = dfb[['dest_ip', 'proto', 'day']].groupby(\n",
    "    ['dest_ip', 'proto'], as_index=False).count().rename({\"day\": \"dest_proto_count\"}, axis=1)\n",
    "# print(destDf.head())\n",
    "\n",
    "src_joined = pd.merge(dfb, srcDf, how='left', on=[\n",
    "                        'src_ip', 'proto'], suffixes=('', '_count'))\n",
    "df3 = pd.merge(src_joined, destDf, how='left', on=[\n",
    "                'dest_ip', 'proto'], suffixes=('', '_count'))\n",
    "\n",
    "df4 = df3.assign(src_proto_pct=df3.src_proto_count / df3.src_count)\n",
    "df5 = df4.assign(dest_proto_pct=df3.dest_proto_count / df3.dest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute Protocol Port Percentages\n",
    "\n",
    "### First compute total protocol counts overall\n",
    "\n",
    "protoDf = df5[['proto', 'src_port']].groupby(\n",
    "    'proto', as_index=False).count().rename({\"src_port\": \"proto_count\"}, axis=1)\n",
    "df6 = pd.merge(df5, protoDf, how='left',\n",
    "                on='proto', suffixes=('', '_count'))\n",
    "\n",
    "protoSPortDf = df6[['proto', 'src_port', 'day']].groupby(\n",
    "    ['proto', 'src_port'], as_index=False).count().rename({\"day\": \"proto_src_port_count\"}, axis=1)\n",
    "df7 = pd.merge(df6, protoSPortDf, how='left', on=[\n",
    "                'proto', 'src_port'], suffixes=('', '_count'))\n",
    "\n",
    "df8 = df7.assign(\n",
    "    proto_src_port_pct=df7.proto_src_port_count/df7.proto_count)\n",
    "\n",
    "print(df8.head())\n",
    "\n",
    "protoDPortDf = df8[['proto', 'dest_port', 'day']].groupby(\n",
    "    ['proto', 'dest_port'], as_index=False).count().rename({\"day\": \"proto_dest_port_count\"}, axis=1)\n",
    "df9 = pd.merge(df8, protoDPortDf, how='left', on=[\n",
    "                'proto', 'dest_port'], suffixes=('', '_count'))\n",
    "\n",
    "df10 = df9.assign(\n",
    "    proto_dest_port_pct=df9.proto_dest_port_count/df9.proto_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standardized counts for number based features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "df10['pkts_scaled'] = scaler.fit_transform(df10[['tot_pkts']])\n",
    "df10['bytes_scaled'] = scaler.fit_transform(df10[['tot_bytes']])\n",
    "df10['duration_scaled'] = scaler.fit_transform(df10[['duration']])\n",
    "\n",
    "df = df10.assign(abs_pkts=abs(df10.pkts_scaled))\n",
    "df = df.assign(abs_bytes=abs(df.bytes_scaled))\n",
    "df = df.assign(abs_dur=abs(df.duration_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureList = ['src_pct', 'dest_pct', 'src_proto_pct', 'dest_proto_pct',\n",
    "                    'proto_src_port_pct', 'proto_dest_port_pct', 'abs_pkts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the full data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the variables for training\n",
    "trainDf = df[featureList]\n",
    "print(trainDf.shape)\n",
    "print(trainDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Outlier Math\n",
    "from scipy import stats\n",
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Example settings\n",
    "n_samples = 100000\n",
    "outliers_fraction = 0.01  # TODO: Tweak this parameter\n",
    "clusters_separation = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the possibility to run multiple outlier detectors\n",
    "# For the purposes of time we will only run Local Outlier Factor\n",
    "# Isolation Forest is another quick and easy one to try\n",
    "classifiers = {\n",
    "    # \"svm\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n",
    "    #                                  kernel=\"rbf\", gamma=0.1),\n",
    "    # \"rc\": EllipticEnvelope(contamination=outliers_fraction),\n",
    "    # \"iso\": IsolationForest(max_samples=n_samples,\n",
    "    #                                     contamination=outliers_fraction,\n",
    "    #                                     random_state=rng),\n",
    "    \"lof\": LocalOutlierFactor(\n",
    "        n_neighbors=25,\n",
    "        contamination=outliers_fraction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Model\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    now = datetime.datetime.now()\n",
    "    print(\"Starting \" + clf_name + \" \" + str(now))\n",
    "    # fit the data and tag outliers\n",
    "    if clf_name == \"lof\":\n",
    "        y_pred = clf.fit_predict(trainDf)\n",
    "        scores_pred = clf.negative_outlier_factor_\n",
    "    else:\n",
    "        clf.fit(trainDf)\n",
    "        scores_pred = clf.decision_function(trainDf)\n",
    "        y_pred = clf.predict(trainDf)\n",
    "    threshold = stats.scoreatpercentile(scores_pred,\n",
    "                                        100 * outliers_fraction)\n",
    "\n",
    "    print(clf_name)\n",
    "    print(threshold)\n",
    "    print(scores_pred)\n",
    "\n",
    "    df[clf_name] = scores_pred\n",
    "    df[clf_name + \"_pred\"] = y_pred\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.size)\n",
    "\n",
    "df.head()\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"Complete \" + str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"lof_pred\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"lof_pred\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
